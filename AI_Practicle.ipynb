{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Practicle 1 - Write and Demonstrate a NLP Program to Word and Text\n",
        "Analysis.\n"
      ],
      "metadata": {
        "id": "4Lj2r2BGIaYc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCQOAG-gDG2f",
        "outputId": "ed8ae33e-daaf-480b-971c-baf28ad50f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "natural: 2\n",
            "language: 2\n",
            "processing: 1\n",
            "fascinating: 1\n",
            "interesting: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is fascinating. Natural language is very interesting\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Convert to lower case\n",
        "tokens = [word.lower() for word in tokens]\n",
        "\n",
        "# Remove punctuation and numbers\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if not word in stop_words]\n",
        "\n",
        "# Frequency distribution\n",
        "freq_dist = nltk.FreqDist(words)\n",
        "\n",
        "# Print 10 most common words\n",
        "for word, frequency in freq_dist.most_common(10):\n",
        "  print(f\"{word}: {frequency}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practicle 2 - Write a program to Implement Word Generation using NLP\n",
        "Technique."
      ],
      "metadata": {
        "id": "QdQJadRBIY0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "nltk.download(\"words\")\n",
        "\n",
        "from nltk.corpus import words\n",
        "\n",
        "class WordGenerator:\n",
        "  def __init__ (self):\n",
        "    self.word_list = set(words.words())\n",
        "\n",
        "  def generate_word(self, length = 6):\n",
        "    word = \"\"\n",
        "    while len(word) != length:\n",
        "      word = random.choice(list(self.word_list))\n",
        "    return word\n",
        "\n",
        "generator = WordGenerator()\n",
        "\n",
        "for _ in range(6):\n",
        "  print(generator.generate_word())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH9DpLueIhGs",
        "outputId": "a3179e35-35b4-42fd-e19d-c21b5c57a2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vannic\n",
            "scutty\n",
            "Wapato\n",
            "bezzle\n",
            "yieldy\n",
            "beelol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practicle 3 - Write a Program to understand the morphology( morphology is the study of the structure and formation of words in a language.) of a word by\n",
        "the use of Add-Delete table."
      ],
      "metadata": {
        "id": "U948ZwKfLMKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "add_delete_table = [\n",
        "    (\"ing\", \"\"),\n",
        "    (\"ed\", \"\"),\n",
        "    (\"\", \"e\"),\n",
        "    (\"er\", \"\"),\n",
        "    (\"est\", \"\"),\n",
        "]\n",
        "\n",
        "def apply_morphology_rule(word, add_delete_table):\n",
        "  morphed_word = []\n",
        "\n",
        "  for rule in add_delete_table:\n",
        "    add, delete = rule\n",
        "\n",
        "    if delete:\n",
        "      if(word.endswith(delete)):\n",
        "        morphed_word.append(word[:-len(delete)] + add)\n",
        "    else:\n",
        "      morphed_word.append(word + add)\n",
        "  return morphed_word\n",
        "\n",
        "word = \"dance\"\n",
        "print(f\"Morphed words for '{word}' : {apply_morphology_rule(word, add_delete_table)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfGw-JmhLN9N",
        "outputId": "c53fc706-5c96-41d1-8ffa-7a3e7bdb0970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Morphed words for 'dance' : ['danceing', 'danceed', 'danc', 'danceer', 'danceest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practicle 4 - Write a Program to learn to calculate bigrams(a bigram is a combination of two adjacent words in a sentence, document, or corpus) from a given\n",
        "corpus(\n",
        "A corpus is a large collection of texts used for studying language patterns and characteristics) and calculate probability of a sentence."
      ],
      "metadata": {
        "id": "w4fAxQUgO1Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "def calculate_bigrams(text):\n",
        "    # Preprocess the text\n",
        "    text = re.sub(r'\\W+', ' ', text.lower())\n",
        "    words = text.split()\n",
        "\n",
        "    # Initialize a dictionary to store the bigrams\n",
        "    bigrams = defaultdict(int)\n",
        "\n",
        "    # Calculate the bigrams\n",
        "    for i in range(len(words) - 1):\n",
        "        bigrams[(words[i], words[i+1])] += 1\n",
        "    return bigrams\n",
        "\n",
        "def calculate_sentence_probability(sentence, bigrams):\n",
        "    # Preprocess the sentence\n",
        "    sentence = re.sub(r'\\W+', ' ', sentence.lower())\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Initialize the probability\n",
        "    probability = 1.0\n",
        "\n",
        "    # Calculate the probability of the sentence\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        bigram_count = bigrams[bigram]\n",
        "        word_count = sum([count for bigram, count in bigrams.items() if bigram[0] == words[i]])\n",
        "        probability *= bigram_count / word_count if word_count > 0 else 0\n",
        "    return probability\n",
        "\n",
        "# Test the functions with a corpus and a sentence\n",
        "corpus = \"The cat sat on the mat. The cat ate the rat. The rat ran away.\"\n",
        "sentence = \"The cat sat on the mat\"\n",
        "bigrams = calculate_bigrams(corpus)\n",
        "probability = calculate_sentence_probability(sentence, bigrams)\n",
        "print(f\"Probability of sentence '{sentence}': {probability}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb-OHOZPO4rU",
        "outputId": "f52649d1-6c42-4600-cf55-5714aea9272d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of sentence 'The cat sat on the mat': 0.04000000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practicle 5 - Demonstrate and design a program to learn how to apply add-\n",
        "one smoothing(Smoothing adjusts probabilities in models to avoid zero probabilities for unseen or low-frequency events, improving model reliability.\n",
        ") on sparse bigram table."
      ],
      "metadata": {
        "id": "RqyszT7WNMLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk import bigrams\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove non-alphabetic characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def calculate_bigrams(corpus):\n",
        "    # Tokenize the preprocessed text into words\n",
        "    words = corpus.split()\n",
        "    # Generate bigrams using NLTK library\n",
        "    bigram_list = list(bigrams(words))\n",
        "    # Count the occurrences of each bigram\n",
        "    bigram_counts = Counter(bigram_list)\n",
        "    return bigram_counts\n",
        "\n",
        "def apply_add_one_smoothing(bigram_counts):\n",
        "    # Create a copy of the bigram counts with added smoothing\n",
        "    smoothed_bigram_counts = bigram_counts.copy()\n",
        "    # Get the vocabulary size (unique bigrams)\n",
        "    vocabulary_size = len(set(bigram_counts.keys()))\n",
        "    # Apply add-one smoothing\n",
        "    for bigram in smoothed_bigram_counts:\n",
        "        smoothed_bigram_counts[bigram] += 1\n",
        "    # Add pseudo-counts for unseen bigrams\n",
        "    smoothed_bigram_counts[None] = vocabulary_size\n",
        "    return smoothed_bigram_counts\n",
        "\n",
        "def main():\n",
        "    # Example corpus\n",
        "    corpus = \"This is a simple example sentence. Another example is also given for demonstration purposes.\"\n",
        "    # Preprocess and calculate bigrams\n",
        "    preprocessed_corpus = preprocess_text(corpus)\n",
        "    bigram_counts = calculate_bigrams(preprocessed_corpus)\n",
        "    # Print the original bigram counts\n",
        "    print(\"Original Bigrams and their counts:\")\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        print(f\"{bigram}: {count}\")\n",
        "    # Apply add-one smoothing\n",
        "    smoothed_bigram_counts = apply_add_one_smoothing(bigram_counts)\n",
        "    # Print the bigram counts after add-one smoothing\n",
        "    print(\"\\nBigrams and their counts after Add-One Smoothing:\")\n",
        "    for bigram, count in smoothed_bigram_counts.items():\n",
        "        print(f\"{bigram}: {count}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2XA3z60NO77",
        "outputId": "3cd5ad9f-1c40-4497-dccd-56ef680fb4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Bigrams and their counts:\n",
            "('this', 'is'): 1\n",
            "('is', 'a'): 1\n",
            "('a', 'simple'): 1\n",
            "('simple', 'example'): 1\n",
            "('example', 'sentence'): 1\n",
            "('sentence', 'another'): 1\n",
            "('another', 'example'): 1\n",
            "('example', 'is'): 1\n",
            "('is', 'also'): 1\n",
            "('also', 'given'): 1\n",
            "('given', 'for'): 1\n",
            "('for', 'demonstration'): 1\n",
            "('demonstration', 'purposes'): 1\n",
            "\n",
            "Bigrams and their counts after Add-One Smoothing:\n",
            "('this', 'is'): 2\n",
            "('is', 'a'): 2\n",
            "('a', 'simple'): 2\n",
            "('simple', 'example'): 2\n",
            "('example', 'sentence'): 2\n",
            "('sentence', 'another'): 2\n",
            "('another', 'example'): 2\n",
            "('example', 'is'): 2\n",
            "('is', 'also'): 2\n",
            "('also', 'given'): 2\n",
            "('given', 'for'): 2\n",
            "('for', 'demonstration'): 2\n",
            "('demonstration', 'purposes'): 2\n",
            "None: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practicle 6 - Write a program to find POS tags of words in a sentence using\n",
        "Viterbi decoding."
      ],
      "metadata": {
        "id": "lleWJJzCO3dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nltk\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Training the Hidden Markov Model (HMM) on the Penn Treebank corpus\n",
        "train_data = treebank.tagged_sents()[:3000]  # Using a subset for training\n",
        "hmm_tagger = nltk.HiddenMarkovModelTagger.train(train_data)\n",
        "\n",
        "def viterbi_decode(sentence, hmm_tagger):\n",
        "    # Tokenize the input sentence\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    # Use the Viterbi algorithm to find the most likely sequence of POS tags\n",
        "    tags = hmm_tagger.tag(words)\n",
        "    return tags\n",
        "\n",
        "# Example usage\n",
        "input_sentence = \"This is a sample sentence.\"\n",
        "result = viterbi_decode(input_sentence, hmm_tagger)\n",
        "# Print the result\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caq_lGGQO6US",
        "outputId": "45355b70-623b-41a0-cec9-388009f0e1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practicle 7 - Calculate emission(For example, the probability of observing the word \"run\" given its POS tag as a verb might be higher compared to its probability as a noun.) and\n",
        "\n",
        "transition matrix(For example, the probability of a noun being followed by a verb might be higher than being followed by an adjective.) which will be helpful\n",
        "for tagging Parts of Speech using Hidden Markov Model."
      ],
      "metadata": {
        "id": "t3F_PDMgQYAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Example tagged sentences\n",
        "tagged_sentences = [\n",
        "    [('The', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')],\n",
        "    [('A', 'DT'), ('dog', 'NN'), ('chases', 'VBZ'), ('the', 'DT'), ('cat', 'NN')]\n",
        "]\n",
        "\n",
        "# Calculate emission matrix\n",
        "def calculate_emission_matrix(tagged_sentences):\n",
        "    emission_matrix = defaultdict(lambda: defaultdict(int))\n",
        "    word_counts = defaultdict(int)\n",
        "\n",
        "    for sentence in tagged_sentences:\n",
        "        for word, tag in sentence:\n",
        "            emission_matrix[tag][word] += 1\n",
        "            word_counts[word] += 1\n",
        "\n",
        "    # Convert counts to probabilities\n",
        "    for tag in emission_matrix:\n",
        "        total_count = sum(emission_matrix[tag].values())\n",
        "        for word in emission_matrix[tag]:\n",
        "            emission_matrix[tag][word] /= total_count\n",
        "\n",
        "    return emission_matrix, word_counts\n",
        "\n",
        "# Calculate transition matrix\n",
        "def calculate_transition_matrix(tagged_sentences):\n",
        "    transition_matrix = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for sentence in tagged_sentences:\n",
        "        prev_tag = None\n",
        "        for _, tag in sentence:\n",
        "            if prev_tag is not None:\n",
        "                transition_matrix[prev_tag][tag] += 1\n",
        "            prev_tag = tag\n",
        "\n",
        "    # Convert counts to probabilities\n",
        "    for prev_tag in transition_matrix:\n",
        "        total_count = sum(transition_matrix[prev_tag].values())\n",
        "        for tag in transition_matrix[prev_tag]:\n",
        "            transition_matrix[prev_tag][tag] /= total_count\n",
        "\n",
        "    return transition_matrix\n",
        "\n",
        "# Example usage\n",
        "emission_matrix, word_counts = calculate_emission_matrix(tagged_sentences)\n",
        "transition_matrix = calculate_transition_matrix(tagged_sentences)\n",
        "\n",
        "# Print emission matrix\n",
        "print(\"Emission Matrix:\")\n",
        "for tag, emissions in emission_matrix.items():\n",
        "    print(tag)\n",
        "    for word, prob in emissions.items():\n",
        "        print(f\"  {word}: {prob:.4f}\")\n",
        "\n",
        "# Print transition matrix\n",
        "print(\"\\nTransition Matrix:\")\n",
        "for prev_tag, transitions in transition_matrix.items():\n",
        "    print(prev_tag)\n",
        "    for tag, prob in transitions.items():\n",
        "        print(f\"  {tag}: {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yRNDS2hQdFC",
        "outputId": "eb8fdbf7-cfde-4249-c794-8155384d7718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emission Matrix:\n",
            "DT\n",
            "  The: 0.2500\n",
            "  the: 0.5000\n",
            "  A: 0.2500\n",
            "NN\n",
            "  cat: 0.5000\n",
            "  mat: 0.2500\n",
            "  dog: 0.2500\n",
            "VBZ\n",
            "  is: 0.5000\n",
            "  chases: 0.5000\n",
            "IN\n",
            "  on: 1.0000\n",
            "\n",
            "Transition Matrix:\n",
            "DT\n",
            "  NN: 1.0000\n",
            "NN\n",
            "  VBZ: 1.0000\n",
            "VBZ\n",
            "  IN: 0.5000\n",
            "  DT: 0.5000\n",
            "IN\n",
            "  DT: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practicle 8 - Design and demonstrate the program to know the importance of\n",
        "selecting proper features for training a model and size of\n",
        "training corpus in learning how to do chunking(identify and label)."
      ],
      "metadata": {
        "id": "nBnbOXKSQywu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('conll2000')\n",
        "from nltk.corpus import conll2000\n",
        "from nltk.chunk.util import conlltags2tree, tree2conlltags\n",
        "from nltk.chunk import ChunkParserI\n",
        "\n",
        "# Define the feature selection function\n",
        "def features(sentence, index):\n",
        "    return {\n",
        "        'word': sentence[index][0],\n",
        "        'postag': sentence[index][1],\n",
        "    }\n",
        "\n",
        "# Define the Chunker class\n",
        "class Chunker(ChunkParserI):\n",
        "    def __init__(self, train_sents):\n",
        "        train_data = [[(t, c) for w, t, c in tree2conlltags(sent)] for sent in train_sents]\n",
        "        self.tagger = nltk.TrigramTagger(train_data)\n",
        "\n",
        "    def parse(self, sentence):\n",
        "        pos_tags = [pos for (word, pos) in sentence]\n",
        "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
        "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
        "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
        "        return conlltags2tree(conlltags)\n",
        "\n",
        "# Load the training and testing data\n",
        "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "\n",
        "# Train the model\n",
        "chunker = Chunker(train_sents)\n",
        "\n",
        "# Test the model\n",
        "print(chunker.evaluate(test_sents))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNNMWIDzQ0vV",
        "outputId": "0234c042-51d2-423b-c24e-d2c980e87fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "<ipython-input-5-84ff6d918b84>:35: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(chunker.evaluate(test_sents))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  93.3%%\n",
            "    Precision:     82.5%%\n",
            "    Recall:        86.8%%\n",
            "    F-Measure:     84.6%%\n"
          ]
        }
      ]
    }
  ]
}